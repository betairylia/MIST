{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "v0_pytorch_sim_knn_geodesic_agm_tau_variable.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeeH4J4pWZKt"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# import os\n",
        "# os.system(\"cp drive/MyDrive/data.zip .\")\n",
        "# os.system(\"unzip data.zip\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCF8kjHw8L03",
        "outputId": "43d58037-4e14-4495-9bd8-296c4c0d8150"
      },
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import cupy as cp\n",
        "from keras.utils import np_utils\n",
        "import copy\n",
        "\n",
        "import contextlib\n",
        "\n",
        "\n",
        "from scipy.sparse import csr_matrix, triu\n",
        "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix, lil_matrix\n",
        "from scipy.sparse import identity\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "import scipy as sp   \n",
        "from scipy.sparse.csgraph import johnson\n",
        "\n",
        "import time\n",
        "\n",
        "!pip install Munkres\n",
        "from munkres import Munkres\n",
        "\n",
        "# by using below command, gpu is available\n",
        "dev = torch.device(\n",
        "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "dsetname = \"unknown\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Munkres in /usr/local/lib/python3.7/dist-packages (1.1.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28T0ewTpscjI"
      },
      "source": [
        "# Helpers\n",
        "def get_custom_feat(data_dir):\n",
        "\n",
        "    data = torch.load(data_dir)\n",
        "    \n",
        "    X = torch.cat([data['trainX'], data['testX']], 0)\n",
        "    Y = torch.cat([data['trainY'], data['testY']], 0)\n",
        "\n",
        "    X = X.reshape(X.shape[0], -1)\n",
        "    Y = Y.reshape(Y.shape[0],)\n",
        "\n",
        "    # dataset = dict()\n",
        "    # dataset['X']=X\n",
        "    # dataset['Y']=Y\n",
        "\n",
        "    # dataloader=DataLoader(TensorDataset(X,Y),batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "\n",
        "    meta = {'dim': X.shape[1], 'nClasses': int(max(Y).item())+1}\n",
        "\n",
        "    return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "\n",
        "def get_custom_feat_npy(data_dir_X, data_dir_Y):\n",
        "\n",
        "    X = torch.FloatTensor(np.load(data_dir_X))\n",
        "    Y = torch.LongTensor(np.load(data_dir_Y))\n",
        "    \n",
        "    X = X.reshape(X.shape[0], -1)\n",
        "    Y = Y.reshape(Y.shape[0],)\n",
        "\n",
        "    # dataset = dict()\n",
        "    # dataset['X']=X\n",
        "    # dataset['Y']=Y\n",
        "\n",
        "    # dataloader=DataLoader(TensorDataset(X,Y),batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "\n",
        "    meta = {'dim': X.shape[1], 'nClasses': int(max(Y).item())+1}\n",
        "\n",
        "    return X, Y, X.shape[0], meta['dim'], meta['nClasses']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5ZZgbUHQ8K3"
      },
      "source": [
        "# # download mnist dataset from keras, then define training dataset X and its true labels Y\n",
        "# from keras.datasets import mnist\n",
        "\n",
        "# (X_tr, Y_tr), (X_tst, Y_tst) = mnist.load_data()\n",
        "\n",
        "# (num_tr, depth, _) = X_tr.shape\n",
        "# (num_tst, _, _) = X_tst.shape\n",
        "\n",
        "# dim = depth**2\n",
        "\n",
        "# X_tr = X_tr.reshape(num_tr, dim) # from 28*28 image to naive 784 dim feature vector \n",
        "# X_tst = X_tst.reshape(num_tst, dim)\n",
        "# X = np.r_[X_tr, X_tst]\n",
        "# print(\"Size of training dataset is\", X.shape)\n",
        "# X = X/255\n",
        "# #X = (2*(X/255)) - 1\n",
        "\n",
        "# Y = np.r_[Y_tr, Y_tst]\n",
        "# Y = Y.reshape(1, Y.shape[0])\n",
        "\n",
        "# C = 10 # the number of classes for MNIST\n",
        "\n",
        "# # print(X.shape)\n",
        "# # print(Y.shape)\n",
        "\n",
        "# dsetname = \"mnist\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlK1vlnoUUW0"
      },
      "source": [
        "# # Make Fshion Mnist dataset\n",
        "# from torchvision.datasets import FashionMNIST\n",
        "\n",
        "# def get_fashion(data_dir = './data/fashion/', batch_size=128, shuffle = True):\n",
        "\n",
        "#     train = FashionMNIST(root = data_dir, train = True, download = True)\n",
        "#     test = FashionMNIST(root = data_dir, train = False, download = True)\n",
        "\n",
        "#     X=torch.cat([train.data.float().view(-1,784)/255.,test.data.float().view(-1,784)/255.],0)\n",
        "#     Y=torch.cat([train.targets,test.targets],0)\n",
        "\n",
        "#     meta = {'dim': 784, 'nClasses': 10}\n",
        "\n",
        "#     return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "# X, Y, _, dim, C = get_fashion()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"f-mnist\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwSxC710qm5o"
      },
      "source": [
        "# # Make 20news dataset\n",
        "\n",
        "# from sklearn.datasets import fetch_20newsgroups, make_circles, make_moons, make_blobs, fetch_openml\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# def get_20news():\n",
        "\n",
        "#     data = fetch_20newsgroups(subset = 'all')\n",
        "#     vectorizer = TfidfVectorizer(max_features = 2000, stop_words = 'english')\n",
        "\n",
        "#     X = vectorizer.fit_transform(data.data).todense().astype(np.float32)\n",
        "#     Y = data.target\n",
        "\n",
        "#     n_samples = X.shape[0]\n",
        "#     dim = X.shape[1]\n",
        "\n",
        "#     Y = Y.reshape(1, n_samples)\n",
        "\n",
        "#     n_classes = 20\n",
        "\n",
        "#     return X, Y, n_samples, dim, n_classes\n",
        "\n",
        "# X, Y, _, dim, C = get_20news()\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"20news\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBDMK9trqv4o"
      },
      "source": [
        "# # Make Reuters10K dataset\n",
        "# def get_reuters10K():\n",
        "#     return get_custom_feat_npy('./IMSAT_datasets/all_dataset/reuters/10k_feature.npy', './IMSAT_datasets/all_dataset/reuters/10k_target.npy')\n",
        "\n",
        "# X, Y, _, dim, C = get_reuters10K()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"reuters10k\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Cv7z7lW4xj"
      },
      "source": [
        "# # Make CIFAR10 dataset\n",
        "# # Features extracted by Zhang following IMSAT paper\n",
        "# def get_CIFAR10_zhang():\n",
        "#     return get_custom_feat('./CIFAR10_feat/data.pkl')\n",
        "\n",
        "# # Features provided with IMSAT code\n",
        "# # Zhang cannot get reported performance with fixed eps; did not try for adaptive epsilon.\n",
        "# def get_CIFAR10_IMSAT():\n",
        "\n",
        "#     PATH = './IMSAT_datasets/all_dataset/cifar/'\n",
        "\n",
        "#     y_train_ul = np.load(PATH+'train_labels.npy').astype(np.int32)\n",
        "#     y_test = np.load(PATH+'test_labels.npy').astype(np.int32)\n",
        "#     y_whole = np.concatenate((y_train_ul, y_test), axis = 0)\n",
        "#     x_whole = np.load(PATH+'resnet.npz')['arr_0']\n",
        "\n",
        "#     X = torch.Tensor(x_whole).reshape(x_whole.shape[0], -1)\n",
        "#     Y = torch.LongTensor(y_whole).reshape(y_whole.shape[0])\n",
        "\n",
        "#     meta = {'dim': X.shape[1], 'nClasses': 10}\n",
        "\n",
        "#     return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "# X, Y, _, dim, C = get_CIFAR10_IMSAT()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"cifar10\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqbCxA2W7zC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6adf505-2ba6-47ae-e2db-c7174c07a8ae"
      },
      "source": [
        "# Make CIFAR100 dataset\n",
        "# Features extracted by Zhang following IMSAT paper\n",
        "def get_CIFAR100_zhang():\n",
        "    return get_custom_feat('./CIFAR100_feat/data.pkl')\n",
        "\n",
        "# Features provided with IMSAT code\n",
        "def get_CIFAR100_IMSAT():\n",
        "\n",
        "    PATH = './IMSAT_datasets/all_dataset/cifar100/'\n",
        "\n",
        "    y_whole = np.load(PATH + 'y.npy')\n",
        "    x_whole = np.load(PATH+'resnet.npz')['arr_0']\n",
        "\n",
        "    X = torch.Tensor(x_whole).reshape(x_whole.shape[0], -1)\n",
        "    Y = torch.LongTensor(y_whole).reshape(y_whole.shape[0])\n",
        "\n",
        "    meta = {'dim': X.shape[1], 'nClasses': 100}\n",
        "\n",
        "    return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "\n",
        "X, Y, _, dim, C = get_CIFAR100_IMSAT()\n",
        "\n",
        "X = X.detach().cpu().numpy()\n",
        "Y = Y.detach().cpu().numpy()\n",
        "Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "print(dim)\n",
        "print(C)\n",
        "print(X.shape[0])\n",
        "\n",
        "dsetname = \"cifar100\""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2048\n",
            "100\n",
            "60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnf_de2uW-6l"
      },
      "source": [
        "# # Make SVHN dataset\n",
        "# def get_svhn():\n",
        "    \n",
        "#     PATH = './IMSAT_datasets/all_dataset/svhn/'\n",
        "\n",
        "#     x_train_ul = np.load(PATH+'train_feature.npy').astype(np.float32)\n",
        "#     x_test = np.load(PATH+'test_feature.npy').astype(np.float32)\n",
        "#     y_train_ul = np.load(PATH+'train_target.npy').astype(np.int32)\n",
        "#     y_test = np.load(PATH+'test_target.npy').astype(np.int32)\n",
        "\n",
        "#     x_whole = np.concatenate((x_train_ul, x_test), axis = 0)\n",
        "#     y_whole = np.concatenate((y_train_ul, y_test), axis = 0)\n",
        "\n",
        "#     X = torch.Tensor(x_whole).reshape(x_whole.shape[0], -1)\n",
        "#     Y = torch.LongTensor(y_whole).reshape(y_whole.shape[0])\n",
        "\n",
        "#     meta = {'dim': X.shape[1], 'nClasses': int(max(Y).item())+1}\n",
        "\n",
        "#     return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "\n",
        "# X, Y, _, dim, C = get_svhn()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"svhn\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV2Tz129XBZC"
      },
      "source": [
        "# # Make Omniglot dataset\n",
        "# def get_omniglot():\n",
        "    \n",
        "#     PATH = './IMSAT_datasets/all_dataset/omniglot/'\n",
        "#     scale=   1.0 / 255.0\n",
        "#     shift= - 0.0\n",
        "\n",
        "#     augmented_data = (np.load(PATH+\"augmented_omniglot_downsampled5_data.npz\")['arr_0']).astype(np.float32)*scale + shift\n",
        "#     augmented_target = np.load(PATH+\"augmented_omniglot_downsampled5_target.npz\")['arr_0']\n",
        "\n",
        "#     X = torch.Tensor(augmented_data).reshape(augmented_data.shape[0], -1)\n",
        "#     Y = torch.LongTensor(augmented_target).reshape(augmented_target.shape[0])\n",
        "\n",
        "#     meta = {'dim': X.shape[1], 'nClasses': int(max(Y).item())+1}\n",
        "\n",
        "#     return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "# X, Y, _, dim, C = get_omniglot()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"omniglot\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_2VSu3pCWBb"
      },
      "source": [
        "# # Make STL dataset\n",
        "\n",
        "# def get_imsat_stl10(batch_size=128):\n",
        "\n",
        "#     PATH = './IMSAT_datasets/all_dataset/stl/'\n",
        "#     label = np.fromfile(PATH+'train_y.bin', dtype=np.uint8)\n",
        "#     test_label = np.fromfile(PATH+'test_y.bin', dtype=np.uint8)\n",
        "#     y_whole = (np.concatenate((label, test_label), axis = 0) - 1).astype(np.int32)\n",
        "#     x_whole = np.load(PATH + 'resnet.npz')['arr_0']\n",
        "#     X = torch.Tensor(x_whole).reshape(x_whole.shape[0], -1)\n",
        "#     Y = torch.LongTensor(y_whole).reshape(y_whole.shape[0])\n",
        "#     meta = {'dim': X.shape[1], 'nClasses': 10}\n",
        "\n",
        "#     return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "# X, Y, _, dim, C = get_imsat_stl10()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"stl\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04UmpQLL0zFV"
      },
      "source": [
        "# # Make Two-Moons\n",
        "# from sklearn.datasets import make_moons\n",
        "\n",
        "# X, Y = make_moons(n_samples=5000,\n",
        "#                   shuffle = None,\n",
        "#                   noise = 0.05,\n",
        "#                   random_state = True,)\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "# dim = 2\n",
        "# C = 2\n",
        "\n",
        "# print(X)\n",
        "# print(Y)\n",
        "\n",
        "# dsetname = \"Two-Moons\"\n",
        "\n",
        "# #Visualization for Two-Moons\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# # Visualization\n",
        "# frame = pd.DataFrame(X)\n",
        "# frame['cluster'] = Y[0]\n",
        "# frame.columns = ['y-var', 'x-var', 'cluster']\n",
        "# color=['red','black']\n",
        "# for j in range(0,C):\n",
        "#     data = frame[frame[\"cluster\"]==j]\n",
        "#     plt.scatter(data[\"y-var\"],data[\"x-var\"],c=color[j],s=1)\n",
        "# plt.show()\n",
        "# print(\"Visualization is finished.\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXtF3O1e235I"
      },
      "source": [
        "# # Make Two-Rings\n",
        "# from sklearn.datasets import make_circles\n",
        "# X, Y = make_circles(n_samples=5000,\n",
        "#                   shuffle = None,\n",
        "#                   noise = 0.01,\n",
        "#                   random_state=True,\n",
        "#                   factor = 0.5)\n",
        "\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "# dim = 2\n",
        "# C = 2\n",
        "\n",
        "# print(X)\n",
        "# print(Y)\n",
        "\n",
        "# dsetname = \"Two-Rings\"\n",
        "\n",
        "# #Visualization for Two-Moons\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "# # Visualization\n",
        "# frame = pd.DataFrame(X)\n",
        "# frame['cluster'] = Y[0]\n",
        "# frame.columns = ['y-var', 'x-var', 'cluster']\n",
        "# color=['red','black']\n",
        "# for j in range(0,C):\n",
        "#     data = frame[frame[\"cluster\"]==j]\n",
        "#     plt.scatter(data[\"y-var\"],data[\"x-var\"],c=color[j],s=1)\n",
        "# plt.show()\n",
        "# print(\"Visualization is finished.\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWDXVqpLLSHi"
      },
      "source": [
        "def Knn2Adj(idx, k):\n",
        "\n",
        "    n = idx.shape[0]\n",
        "\n",
        "    idx_i = np.array(range(n))\n",
        "    idx_i = np.tile(idx_i, (k+1,1)).transpose().reshape(1, n*(k+1))[0]\n",
        "    idx_j = idx.reshape(1, n*(k+1))[0]\n",
        "\n",
        "    adj = csc_matrix((np.ones(n*(k+1)),(idx_i,idx_j)), shape=(n,n)).astype(float)\n",
        "    e = identity(n)\n",
        "    adj = adj - e\n",
        "    adj = adj + adj.transpose() - adj.multiply(adj.transpose())\n",
        "\n",
        "    del idx_i, idx_j, e\n",
        "\n",
        "    return adj\n",
        "\n",
        "\n",
        "\n",
        "def build_gnng(K_g, indices):\n",
        "    KnnIDXwithKg = indices[:,0:K_g+1]\n",
        "    A = Knn2Adj(KnnIDXwithKg, K_g)\n",
        "\n",
        "    start = time.time()\n",
        "    # geodesic_dist = csc_matrix(sp.sparse.csgraph.johnson(A))\n",
        "    g_distances = sp.sparse.csgraph.johnson(A)\n",
        "    del A\n",
        "    g_distances = np.where( (g_distances==np.inf) | (g_distances==0), 0, g_distances) \n",
        "    m = np.min(np.count_nonzero(g_distances, axis=1))\n",
        "    n = g_distances.shape[1]\n",
        "    g_indices = np.argsort(g_distances, axis=0)\n",
        "    #g_indices = g_indices[:,n-K_g:n]\n",
        "    g_indices = g_indices[:,n-m:n]\n",
        "    g_distances = csc_matrix(g_distances)\n",
        "    #del geodesic_dist\n",
        "    #geodesic_dist = np.where(geodesic_dist>0, 1, geodesic_dist)\n",
        "    #geodesic_nng = geodesic_dist.nonzero()\n",
        "    end = time.time()\n",
        "    print(f\"{end-start} seconds by computing geodesic distance.\")\n",
        "\n",
        "    return g_distances, g_indices\n",
        "\n",
        "\n",
        "def conditional_entropy(soft):\n",
        "    loss = torch.sum(-soft*torch.log(soft + 1e-8)) / soft.shape[0]\n",
        "    return loss\n",
        "\n",
        "\n",
        "def entropy(soft):\n",
        "    avg_soft = torch.mean(soft, 0, True) \n",
        "    loss = -torch.sum(avg_soft * torch.log(avg_soft + 1e-8))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def kl(p, q):\n",
        "    loss = torch.sum(p * torch.log((p + 1e-8) / (q + 1e-8))) / p.shape[0]\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def _disable_tracking_bn_stats(model):\n",
        "\n",
        "    def switch_attr(m):\n",
        "        if hasattr(m, 'track_running_stats'):\n",
        "            m.track_running_stats ^= True\n",
        "            \n",
        "    model.apply(switch_attr)\n",
        "    yield\n",
        "    model.apply(switch_attr)\n",
        "\n",
        "\n",
        "def _l2_normalize(d):\n",
        "    d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
        "    d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-8\n",
        "    return d\n",
        "\n",
        "\n",
        "def return_vat_Loss(model, x, xi, eps):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    with _disable_tracking_bn_stats(model):\n",
        "        with torch.no_grad():\n",
        "            target = torch.softmax(model(x), 1) \n",
        "        \n",
        "        d = torch.randn(x.shape).to(dev)\n",
        "        d = _l2_normalize(d)\n",
        "        d.requires_grad_()\n",
        "        out_vadv = model(x + xi*d)\n",
        "        hat = torch.softmax(out_vadv, 1)\n",
        "        adv_distance = kl(target, hat)\n",
        "\n",
        "        adv_distance.backward()\n",
        "        \n",
        "        d = _l2_normalize(d.grad)\n",
        "        r_adv = eps * d\n",
        "        out_vadv = model(x + r_adv)\n",
        "        hat = torch.softmax(out_vadv, 1)\n",
        "        R_vat = kl(target, hat)\n",
        "\n",
        "    return R_vat\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## upper-bound of I_nce\n",
        "# def SiameseLoss(model, soft_out1, x_agm, t1, alpha):\n",
        "\n",
        "#     m = soft_out1.shape[0]\n",
        "#     a = csr_matrix(np.ones((m,m)))\n",
        "   \n",
        "#     idx_i, idx_j = a.nonzero()\n",
        "#     del a\n",
        "\n",
        "     \n",
        "#     with _disable_tracking_bn_stats(model):\n",
        "#         soft_out2 = torch.softmax(model(x_agm), 1)\n",
        "\n",
        "#         ip = soft_out1[idx_i,:] * soft_out2[idx_j,:]\n",
        "#         ip = torch.sum(ip, 1).reshape(m,m)\n",
        "\n",
        "#         if alpha==0:\n",
        "#           critic = torch.log( 1 + t1*(ip - 1) )\n",
        "\n",
        "#         if alpha==1:\n",
        "#           critic = t1 * ip\n",
        "          \n",
        "#         if alpha==2:\n",
        "#           critic = -torch.log( 1 + t1*(1-ip) )\n",
        "\n",
        "#         lp = torch.mean(torch.diag(critic,0)) \n",
        "#         ln = -torch.sum(torch.tril(critic,-1) + torch.triu(critic,1)) /(m*(m-1))\n",
        "#         l_s = (1 - (1/m))*(lp + ln)\n",
        "\n",
        "#     return l_s, soft_out2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## exact I_nce\n",
        "# def SiameseLoss(model, soft_out1, x_agm, t1, alpha):\n",
        "\n",
        "#     m = soft_out1.shape[0]\n",
        "#     a = csr_matrix(np.ones((m,m)))\n",
        "   \n",
        "#     idx_i, idx_j = a.nonzero()\n",
        "#     del a\n",
        "\n",
        "     \n",
        "#     with _disable_tracking_bn_stats(model):\n",
        "#         soft_out2 = torch.softmax(model(x_agm), 1)\n",
        "\n",
        "#         ip = soft_out1[idx_i,:] * soft_out2[idx_j,:]\n",
        "#         ip = torch.sum(ip, 1).reshape(m,m)\n",
        "\n",
        "#         if alpha==0:\n",
        "#           critic = torch.log( 1 + t1*(ip - 1) )\n",
        "\n",
        "#         if alpha==1:\n",
        "#           critic = t1 * ip\n",
        "          \n",
        "#         if alpha==2:\n",
        "#           critic = -torch.log( 1 + t1*(1-ip) )\n",
        "\n",
        "\n",
        "#         I_nce = np.log(m) + torch.mean( torch.log( torch.diag( torch.softmax(critic,1),0 ) ) )\n",
        "\n",
        "#         l_s = I_nce\n",
        "\n",
        "#     return l_s, soft_out2\n",
        "\n",
        "\n",
        "\n",
        "## symmetrized I_nce\n",
        "def SiameseLoss(model, soft_out1, x_agm, t1, alpha):\n",
        "\n",
        "    m = soft_out1.shape[0]\n",
        "    a = csr_matrix(np.ones((m,m)))\n",
        "   \n",
        "    idx_i, idx_j = a.nonzero()\n",
        "    del a\n",
        "\n",
        "     \n",
        "    with _disable_tracking_bn_stats(model):\n",
        "        soft_out2 = torch.softmax(model(x_agm), 1)\n",
        "\n",
        "        ip = soft_out1[idx_i,:] * soft_out2[idx_j,:]\n",
        "        ip = torch.sum(ip, 1).reshape(m,m)\n",
        "\n",
        "        if alpha==0:\n",
        "          critic = torch.log( 1 + t1*(ip - 1) )\n",
        "\n",
        "        if alpha==1:\n",
        "          critic = t1 * ip\n",
        "          \n",
        "        if alpha==2:\n",
        "          critic = -torch.log( 1 + t1*(1-ip) )\n",
        "        \n",
        "\n",
        "        I_nce1 = np.log(m) + torch.mean( torch.log( torch.diag( torch.softmax(critic,1),0 ) ) )\n",
        "        I_nce0 = np.log(m) + torch.mean( torch.log( torch.diag( torch.softmax(critic,0),0 ) ) )\n",
        "\n",
        "\n",
        "        l_s = (I_nce1 + I_nce0) / 2\n",
        "\n",
        "    return l_s, soft_out2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ReturnACC(cluster, target_cluster, k):\n",
        "    \"\"\" Compute error between cluster and target cluster\n",
        "    :param cluster: proposed cluster\n",
        "    :param target_cluster: target cluster\n",
        "    k: number of classes\n",
        "    :return: error\n",
        "    \"\"\"\n",
        "    n = np.shape(target_cluster)[0]\n",
        "    M = np.zeros((k, k))\n",
        "    for i in range(k):\n",
        "        for j in range(k):\n",
        "            M[i][j] = np.sum(np.logical_and(cluster == i, target_cluster == j))\n",
        "    m = Munkres()\n",
        "    indexes = m.compute(-M)\n",
        "    corresp = []\n",
        "    for i in range(k):\n",
        "        corresp.append(indexes[i][1])\n",
        "    pred_corresp = [corresp[int(predicted)] for predicted in cluster]\n",
        "    acc = np.sum(pred_corresp == target_cluster) / float(len(target_cluster))\n",
        "    return acc \n",
        "\n",
        "\n",
        "\n",
        "def get_agm(x, idx_itr, knn_idx):\n",
        "    knn_idx_itr = knn_idx[idx_itr,:]\n",
        "    for i in range(knn_idx_itr.shape[0]):\n",
        "      v = np.random.permutation(knn_idx_itr[i,:])\n",
        "      knn_idx_itr[i,:] = v\n",
        "\n",
        "    v = np.random.choice(list(range(knn_idx_itr.shape[1])), size=1, replace=False)\n",
        "    idx0 = knn_idx_itr[:,v[0]]\n",
        "    x_agm0 = x[idx0,:]\n",
        "    \n",
        "    return x_agm0, idx0\n",
        "\n",
        "\n",
        "\n",
        "def minimize_tau(model, x, x_agm):\n",
        "    from scipy.special import softmax\n",
        "    from scipy.optimize import minimize_scalar\n",
        "\n",
        "\n",
        "    m = x.shape[0]\n",
        "    a = csr_matrix(np.ones((m,m)))\n",
        "\n",
        "    idx_i, idx_j = a.nonzero()\n",
        "    del a\n",
        "\n",
        "    with _disable_tracking_bn_stats(model):\n",
        "        with torch.no_grad():\n",
        "            y = torch.softmax(model(x), 1)\n",
        "            y_agm = torch.softmax(model(x_agm), 1)\n",
        "            ip = y[idx_i,:] * y_agm[idx_j,:]\n",
        "            ip = torch.sum(ip, 1).reshape(m,m)\n",
        "            ip = ip.cpu().numpy()\n",
        "    \n",
        "    ##alpha=1\n",
        "    def f(t, ip):\n",
        "        critic = t * ip\n",
        "        I1 = np.mean( np.log( np.diag( softmax(critic,1),0 ) ) ) \n",
        "        I0 = np.mean( np.log( np.diag( softmax(critic,0),0 ) ) )\n",
        "        l_s = (I1 + I0) / 2\n",
        "        return -l_s\n",
        "    \n",
        "    res = minimize_scalar(f, args=ip, bounds=(0, 10), method='bounded')\n",
        "    #res = minimize_scalar(f, args=ip, method='golden')\n",
        "    tau = res.x\n",
        "\n",
        "    return tau"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeTt8H309lNv",
        "outputId": "88f84932-c04e-4f82-fa25-dcbc59217224"
      },
      "source": [
        "import os\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "alpha_vat = 0.25\n",
        "K = 10 # Number of neighbors for constructing knn graph\n",
        "\n",
        "\n",
        "knncachestr = \"%s-k%d.npy\" % (dsetname, K+1)\n",
        "\n",
        "if dsetname != \"unknown\" and os.path.exists(knncachestr):\n",
        "  \n",
        "  print(\"Loaded cached kNN from %s\" % knncachestr)\n",
        "  distances = np.load(knncachestr)\n",
        "  indices = np.load(\"i\" + knncachestr)\n",
        "\n",
        "else:\n",
        "\n",
        "  nbrs = NearestNeighbors(n_neighbors=K+1, algorithm='brute').fit(X)\n",
        "  distances, indices = nbrs.kneighbors(X)\n",
        "  np.save(knncachestr, distances)\n",
        "  np.save(\"i\" + knncachestr, indices)\n",
        "\n",
        "K_vat = 10\n",
        "R = alpha_vat*distances[:,K_vat]\n",
        "R = R.reshape(X.shape[0],1)\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print(f\"{end-start} seconds by Brute-Force KNN.\")\n",
        "\n",
        "\n",
        "R = torch.tensor(R.astype('f')).to(dev)\n",
        "X = torch.tensor(X.astype('f')).to(dev) # this unlabeled dataset (set of feature vectors) is input"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded cached kNN from cifar100-k11.npy\n",
            "0.007469654083251953 seconds by Brute-Force KNN.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM6hsDn18kgQ",
        "outputId": "2201452f-bbc2-48a2-e348-dfa24386fe37"
      },
      "source": [
        "# define archtechture of MLP(Multi Layer Perceptron). \n",
        "# in this net, batch-normalization (bn) is used. \n",
        "# bn is very important to stabilize the training of net. \n",
        "#torch.manual_seed(0)\n",
        "class Net(nn.Module): \n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(dim, 1200)\n",
        "        self.bn1 = nn.BatchNorm1d(1200)\n",
        "\n",
        "        self.l2 = nn.Linear(1200,1200)\n",
        "        self.bn2 = nn.BatchNorm1d(1200)\n",
        "\n",
        "        self.l3 = nn.Linear(1200,C)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = F.relu(self.bn1(self.l1(x)))\n",
        "        x = F.relu(self.bn2(self.l2(x)))\n",
        "        x = self.l3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# throw net object to gpu\n",
        "net = net.to(dev)\n",
        "\n",
        "\n",
        "################################################## Training of imsat ############################################\n",
        "\n",
        "# decide hyperparameter values for imsat training\n",
        "epochs = 50 # number of epochs\n",
        "\n",
        "xi = 10**(1)       # xi is used for making adversarial vector. if xi becomes smaller, \n",
        "                   # theoretically obtained r_vadv becomes more priecise\n",
        "\n",
        "\n",
        "mini_size = 250 # size of mini-batch training dataset\n",
        "\n",
        "m = X.shape[0]//mini_size # number of iteration at each epoch \n",
        "\n",
        "\n",
        "mu = 0.045\n",
        "gamma = 1.5\n",
        "eta = 5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## define optimizer for set of parameters in deep neural network\n",
        "## lr is the learning rate of parameter vector, betas are the lr of gradient and second moment of gradient\n",
        "optimizer = optim.Adam(net.parameters(), \n",
        "                        lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "\n",
        "tau = 0.05 ## initial tau\n",
        "\n",
        "alpha = 1 ## alpha-exp family -> choose alpha = 0 or 1 or 2\n",
        "\n",
        "\n",
        "#should be smaller than K=200, here we define T(x) for each x, K0 = 5, 7, 15, 100, 200(=K)\n",
        "#mnist, svhn, stl, omniglot, cifar100 -> K0=7\n",
        "#reuters10k -> K0=50\n",
        "#20news -> K0=200\n",
        "#cifar10 -> K0=15\n",
        "K0 = 7\n",
        "\n",
        "\n",
        "beta = 0 ## 20news, reuters10K -> beta=4/5, the others -> beta=0\n",
        "\n",
        "\n",
        "print(\"Start training of SIM_agm.\")\n",
        "for epoch in range(epochs):\n",
        "    print(\"At \", epoch, \"-th epoch, \")\n",
        "\n",
        "    # set empiricial loss to 0 in the beginning of epoch\n",
        "    empirical_objective_loss = 0.0\n",
        "\n",
        "    idx_eph = np.random.permutation(X.shape[0])\n",
        "    \n",
        "    net.train()\n",
        "\n",
        "    for itr in range(m):\n",
        "      \n",
        "      ## chose a core idx of mini_batch\n",
        "      idx_itr = idx_eph[itr*mini_size:(itr+1)*mini_size]\n",
        "\n",
        "      # define components at each iteration\n",
        "      X_itr = X[idx_itr,:]\n",
        "\n",
        "\n",
        "      ########################## transformation function related part ##############\n",
        "      if beta == 0:\n",
        "        X_agm1_itr, idx_agm1_itr = get_agm(X, idx_itr, indices[:,1:K0+1]) #except 20news reuters10k\n",
        "      else:\n",
        "        X_agm1_itr, idx_agm1_itr = get_agm(X, idx_itr, indices[:,int(beta*K0):K0+1]) #20news or reuters10k\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      #################### VAT loss ################\n",
        "      l_vat = return_vat_Loss(net, X_itr, xi, R[idx_itr,:])\n",
        "      l_vat1 = return_vat_Loss(net, X_agm1_itr, xi, R[idx_agm1_itr,:])\n",
        "\n",
        "\n",
        "\n",
        "      soft_out_itr = torch.softmax(net(X_itr) , 1)\n",
        "  \n",
        "\n",
        "\n",
        "      ####################### Siamese loss (or I_nce) related part ###########\n",
        "      ## define positive and negative loss, where tau is a fixed value\n",
        "      l_s, soft_out_agm1_itr = SiameseLoss(net, soft_out_itr, X_agm1_itr, tau, alpha)\n",
        "      \n",
        "\n",
        "\n",
        "      ######################## I(X;Y) related part #######################\n",
        "      ## define entropy of y\n",
        "      ent_y = entropy(torch.cat((soft_out_itr, soft_out_agm1_itr),0))\n",
        "      \n",
        "\n",
        "      ## define shannon conditional entropy loss H(p(y|x)) named by c_ent.\n",
        "      c_ent = conditional_entropy(torch.cat((soft_out_itr, soft_out_agm1_itr),0))\n",
        "      \n",
        "\n",
        "\n",
        "      ######################### our objective defined ##################        \n",
        "      # objective of sim\n",
        "      objective = ((l_vat + l_vat1)/2) - mu*(  (eta*ent_y - c_ent) + gamma*( l_s )  )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # update the set of parameters in deep neural network by minimizing loss\n",
        "      optimizer.zero_grad() \n",
        "      objective.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "      empirical_objective_loss = empirical_objective_loss + objective.data\n",
        "\n",
        "      tau = minimize_tau(net, X_itr, X_agm1_itr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # #empirical_loss = running_loss/m\n",
        "    empirical_objective_loss = empirical_objective_loss.cpu().numpy()\n",
        "    print(\"average empirical objective loss is\", empirical_objective_loss/m, ',')\n",
        "    print(\"tau is now\", tau, \".\")\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    # at each epoch, prediction accuracy is displayed\n",
        "    with torch.no_grad():\n",
        "        out = net(X)\n",
        "        preds = torch.argmax(out, dim=1)\n",
        "        preds = preds.cpu().numpy()\n",
        "        preds = preds.reshape(1, preds.shape[0])\n",
        "        clustering_acc = ReturnACC(preds[0], Y[0], C)\n",
        "    print(\"and current clustering accuracy is\", clustering_acc )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training of SIM_agm.\n",
            "At  0 -th epoch, \n",
            "average empirical objective loss is -0.8021390279134114 ,\n",
            "tau is now 9.999986212037934 .\n",
            "and current clustering accuracy is 0.17603333333333335\n",
            "At  1 -th epoch, \n",
            "average empirical objective loss is -0.8192709604899089 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.20873333333333333\n",
            "At  2 -th epoch, \n",
            "average empirical objective loss is -0.8340084711710612 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.22695\n",
            "At  3 -th epoch, \n",
            "average empirical objective loss is -0.8445523579915365 ,\n",
            "tau is now 9.999993455003448 .\n",
            "and current clustering accuracy is 0.24201666666666666\n",
            "At  4 -th epoch, \n",
            "average empirical objective loss is -0.8521704991658529 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.24588333333333334\n",
            "At  5 -th epoch, \n",
            "average empirical objective loss is -0.8584838231404622 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.2494\n",
            "At  6 -th epoch, \n",
            "average empirical objective loss is -0.8638325373331706 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.25153333333333333\n",
            "At  7 -th epoch, \n",
            "average empirical objective loss is -0.8683860778808594 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.2602333333333333\n",
            "At  8 -th epoch, \n",
            "average empirical objective loss is -0.8721951802571615 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.26055\n",
            "At  9 -th epoch, \n",
            "average empirical objective loss is -0.87519957224528 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.25983333333333336\n",
            "At  10 -th epoch, \n",
            "average empirical objective loss is -0.8787634531656902 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.2659666666666667\n",
            "At  11 -th epoch, \n",
            "average empirical objective loss is -0.882014274597168 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.26206666666666667\n",
            "At  12 -th epoch, \n",
            "average empirical objective loss is -0.8836051305135091 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.26385\n",
            "At  13 -th epoch, \n",
            "average empirical objective loss is -0.8871844609578451 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.26871666666666666\n",
            "At  14 -th epoch, \n",
            "average empirical objective loss is -0.889485232035319 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.26538333333333336\n",
            "At  15 -th epoch, \n",
            "average empirical objective loss is -0.8908814748128255 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.26475\n",
            "At  16 -th epoch, \n",
            "average empirical objective loss is -0.8938175201416015 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.26293333333333335\n",
            "At  17 -th epoch, \n",
            "average empirical objective loss is -0.8950799942016602 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.2679166666666667\n",
            "At  18 -th epoch, \n",
            "average empirical objective loss is -0.896029281616211 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.2684166666666667\n",
            "At  19 -th epoch, \n",
            "average empirical objective loss is -0.8979621251424154 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.2723333333333333\n",
            "At  20 -th epoch, \n",
            "average empirical objective loss is -0.8994532267252604 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.2689\n",
            "At  21 -th epoch, \n",
            "average empirical objective loss is -0.900860341389974 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.27035\n",
            "At  22 -th epoch, \n",
            "average empirical objective loss is -0.9011173884073893 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.27216666666666667\n",
            "At  23 -th epoch, \n",
            "average empirical objective loss is -0.9044226964314779 ,\n",
            "tau is now 9.9999947848782 .\n",
            "and current clustering accuracy is 0.2723833333333333\n",
            "At  24 -th epoch, \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}