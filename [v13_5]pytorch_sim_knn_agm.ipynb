{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[v13_5]pytorch_sim_knn_agm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeeH4J4pWZKt"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# import os\n",
        "# os.system(\"cp drive/MyDrive/data.zip .\")\n",
        "# os.system(\"unzip data.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCF8kjHw8L03",
        "outputId": "edcd032a-ca6b-4a18-a90d-fd5d817558f2"
      },
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import cupy as cp\n",
        "from keras.utils import np_utils\n",
        "import copy\n",
        "\n",
        "import contextlib\n",
        "\n",
        "\n",
        "from scipy.sparse import csr_matrix, triu\n",
        "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix, lil_matrix\n",
        "from scipy.sparse import identity\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "import time\n",
        "\n",
        "!pip install Munkres\n",
        "from munkres import Munkres\n",
        "\n",
        "# by using below command, gpu is available\n",
        "dev = torch.device(\n",
        "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "dsetname = \"unknown\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Munkres in /usr/local/lib/python3.7/dist-packages (1.1.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28T0ewTpscjI"
      },
      "source": [
        "# Helpers\n",
        "def get_custom_feat(data_dir):\n",
        "\n",
        "    data = torch.load(data_dir)\n",
        "    \n",
        "    X = torch.cat([data['trainX'], data['testX']], 0)\n",
        "    Y = torch.cat([data['trainY'], data['testY']], 0)\n",
        "\n",
        "    X = X.reshape(X.shape[0], -1)\n",
        "    Y = Y.reshape(Y.shape[0],)\n",
        "\n",
        "    # dataset = dict()\n",
        "    # dataset['X']=X\n",
        "    # dataset['Y']=Y\n",
        "\n",
        "    # dataloader=DataLoader(TensorDataset(X,Y),batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "\n",
        "    meta = {'dim': X.shape[1], 'nClasses': int(max(Y).item())+1}\n",
        "\n",
        "    return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "\n",
        "def get_custom_feat_npy(data_dir_X, data_dir_Y):\n",
        "\n",
        "    X = torch.FloatTensor(np.load(data_dir_X))\n",
        "    Y = torch.LongTensor(np.load(data_dir_Y))\n",
        "    \n",
        "    X = X.reshape(X.shape[0], -1)\n",
        "    Y = Y.reshape(Y.shape[0],)\n",
        "\n",
        "    # dataset = dict()\n",
        "    # dataset['X']=X\n",
        "    # dataset['Y']=Y\n",
        "\n",
        "    # dataloader=DataLoader(TensorDataset(X,Y),batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "\n",
        "    meta = {'dim': X.shape[1], 'nClasses': int(max(Y).item())+1}\n",
        "\n",
        "    return X, Y, X.shape[0], meta['dim'], meta['nClasses']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5ZZgbUHQ8K3"
      },
      "source": [
        "# # download mnist dataset from keras, then define training dataset X and its true labels Y\n",
        "# from keras.datasets import mnist\n",
        "\n",
        "# (X_tr, Y_tr), (X_tst, Y_tst) = mnist.load_data()\n",
        "\n",
        "# (num_tr, depth, _) = X_tr.shape\n",
        "# (num_tst, _, _) = X_tst.shape\n",
        "\n",
        "# dim = depth**2\n",
        "\n",
        "# X_tr = X_tr.reshape(num_tr, dim) # from 28*28 image to naive 784 dim feature vector \n",
        "# X_tst = X_tst.reshape(num_tst, dim)\n",
        "# X = np.r_[X_tr, X_tst]\n",
        "# print(\"Size of training dataset is\", X.shape)\n",
        "# X = X/255\n",
        "# #X = (2*(X/255)) - 1\n",
        "\n",
        "# Y = np.r_[Y_tr, Y_tst]\n",
        "# Y = Y.reshape(1, Y.shape[0])\n",
        "\n",
        "# C = 10 # the number of classes for MNIST\n",
        "\n",
        "# # print(X.shape)\n",
        "# # print(Y.shape)\n",
        "\n",
        "# dsetname = \"mnist\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlK1vlnoUUW0"
      },
      "source": [
        "# # Make Fshion Mnist dataset\n",
        "# from torchvision.datasets import FashionMNIST\n",
        "\n",
        "# def get_fashion(data_dir = './data/fashion/', batch_size=128, shuffle = True):\n",
        "\n",
        "#     train = FashionMNIST(root = data_dir, train = True, download = True)\n",
        "#     test = FashionMNIST(root = data_dir, train = False, download = True)\n",
        "\n",
        "#     X=torch.cat([train.data.float().view(-1,784)/255.,test.data.float().view(-1,784)/255.],0)\n",
        "#     Y=torch.cat([train.targets,test.targets],0)\n",
        "\n",
        "#     meta = {'dim': 784, 'nClasses': 10}\n",
        "\n",
        "#     return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "# X, Y, _, dim, C = get_fashion()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"f-mnist\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwSxC710qm5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea107ac-8865-44c2-dc61-a37812bc7490"
      },
      "source": [
        "# Make 20news dataset\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups, make_circles, make_moons, make_blobs, fetch_openml\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "def get_20news():\n",
        "\n",
        "    data = fetch_20newsgroups(subset = 'all')\n",
        "    vectorizer = TfidfVectorizer(max_features = 2000, stop_words = 'english')\n",
        "\n",
        "    X = vectorizer.fit_transform(data.data).todense().astype(np.float32)\n",
        "    Y = data.target\n",
        "\n",
        "    n_samples = X.shape[0]\n",
        "    dim = X.shape[1]\n",
        "\n",
        "    Y = Y.reshape(1, n_samples)\n",
        "\n",
        "    n_classes = 20\n",
        "\n",
        "    return X, Y, n_samples, dim, n_classes\n",
        "\n",
        "X, Y, _, dim, C = get_20news()\n",
        "\n",
        "print(dim)\n",
        "print(C)\n",
        "print(X.shape[0])\n",
        "\n",
        "dsetname = \"20news\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "20\n",
            "18846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBDMK9trqv4o"
      },
      "source": [
        "# # Make Reuters10K dataset\n",
        "# def get_reuters10K():\n",
        "#     return get_custom_feat_npy('./IMSAT_datasets/all_dataset/reuters/10k_feature.npy', './IMSAT_datasets/all_dataset/reuters/10k_target.npy')\n",
        "\n",
        "# X, Y, _, dim, C = get_reuters10K()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"reuters10k\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7Cv7z7lW4xj"
      },
      "source": [
        "# # Make CIFAR10 dataset\n",
        "# # Features extracted by Zhang following IMSAT paper\n",
        "# def get_CIFAR10_zhang():\n",
        "#     return get_custom_feat('./CIFAR10_feat/data.pkl')\n",
        "\n",
        "# # Features provided with IMSAT code\n",
        "# # Zhang cannot get reported performance with fixed eps; did not try for adaptive epsilon.\n",
        "# def get_CIFAR10_IMSAT():\n",
        "\n",
        "#     PATH = './IMSAT_datasets/all_dataset/cifar/'\n",
        "\n",
        "#     y_train_ul = np.load(PATH+'train_labels.npy').astype(np.int32)\n",
        "#     y_test = np.load(PATH+'test_labels.npy').astype(np.int32)\n",
        "#     y_whole = np.concatenate((y_train_ul, y_test), axis = 0)\n",
        "#     x_whole = np.load(PATH+'resnet.npz')['arr_0']\n",
        "\n",
        "#     X = torch.Tensor(x_whole).reshape(x_whole.shape[0], -1)\n",
        "#     Y = torch.LongTensor(y_whole).reshape(y_whole.shape[0])\n",
        "\n",
        "#     meta = {'dim': X.shape[1], 'nClasses': 10}\n",
        "\n",
        "#     return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "# X, Y, _, dim, C = get_CIFAR10_IMSAT()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"cifar10\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqbCxA2W7zC"
      },
      "source": [
        "# # Make CIFAR100 dataset\n",
        "# # Features extracted by Zhang following IMSAT paper\n",
        "# def get_CIFAR100_zhang():\n",
        "#     return get_custom_feat('./CIFAR100_feat/data.pkl')\n",
        "\n",
        "# # Features provided with IMSAT code\n",
        "# def get_CIFAR100_IMSAT():\n",
        "\n",
        "#     PATH = './IMSAT_datasets/all_dataset/cifar100/'\n",
        "\n",
        "#     y_whole = np.load(PATH + 'y.npy')\n",
        "#     x_whole = np.load(PATH+'resnet.npz')['arr_0']\n",
        "\n",
        "#     X = torch.Tensor(x_whole).reshape(x_whole.shape[0], -1)\n",
        "#     Y = torch.LongTensor(y_whole).reshape(y_whole.shape[0])\n",
        "\n",
        "#     meta = {'dim': X.shape[1], 'nClasses': 100}\n",
        "\n",
        "#     return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "\n",
        "# X, Y, _, dim, C = get_CIFAR100_IMSAT()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"cifar100\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnf_de2uW-6l"
      },
      "source": [
        "# # Make SVHN dataset\n",
        "# def get_svhn():\n",
        "    \n",
        "#     PATH = './IMSAT_datasets/all_dataset/svhn/'\n",
        "\n",
        "#     x_train_ul = np.load(PATH+'train_feature.npy').astype(np.float32)\n",
        "#     x_test = np.load(PATH+'test_feature.npy').astype(np.float32)\n",
        "#     y_train_ul = np.load(PATH+'train_target.npy').astype(np.int32)\n",
        "#     y_test = np.load(PATH+'test_target.npy').astype(np.int32)\n",
        "\n",
        "#     x_whole = np.concatenate((x_train_ul, x_test), axis = 0)\n",
        "#     y_whole = np.concatenate((y_train_ul, y_test), axis = 0)\n",
        "\n",
        "#     X = torch.Tensor(x_whole).reshape(x_whole.shape[0], -1)\n",
        "#     Y = torch.LongTensor(y_whole).reshape(y_whole.shape[0])\n",
        "\n",
        "#     meta = {'dim': X.shape[1], 'nClasses': int(max(Y).item())+1}\n",
        "\n",
        "#     return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "\n",
        "# X, Y, _, dim, C = get_svhn()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"svhn\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV2Tz129XBZC"
      },
      "source": [
        "# # Make Omniglot dataset\n",
        "# def get_omniglot():\n",
        "    \n",
        "#     PATH = './IMSAT_datasets/all_dataset/omniglot/'\n",
        "#     scale=   1.0 / 255.0\n",
        "#     shift= - 0.0\n",
        "\n",
        "#     augmented_data = (np.load(PATH+\"augmented_omniglot_downsampled5_data.npz\")['arr_0']).astype(np.float32)*scale + shift\n",
        "#     augmented_target = np.load(PATH+\"augmented_omniglot_downsampled5_target.npz\")['arr_0']\n",
        "\n",
        "#     X = torch.Tensor(augmented_data).reshape(augmented_data.shape[0], -1)\n",
        "#     Y = torch.LongTensor(augmented_target).reshape(augmented_target.shape[0])\n",
        "\n",
        "#     meta = {'dim': X.shape[1], 'nClasses': int(max(Y).item())+1}\n",
        "\n",
        "#     return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "# X, Y, _, dim, C = get_omniglot()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"omniglot\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_2VSu3pCWBb"
      },
      "source": [
        "# # Make STL dataset\n",
        "\n",
        "# def get_imsat_stl10(batch_size=128):\n",
        "\n",
        "#     PATH = './IMSAT_datasets/all_dataset/stl/'\n",
        "#     label = np.fromfile(PATH+'train_y.bin', dtype=np.uint8)\n",
        "#     test_label = np.fromfile(PATH+'test_y.bin', dtype=np.uint8)\n",
        "#     y_whole = (np.concatenate((label, test_label), axis = 0) - 1).astype(np.int32)\n",
        "#     x_whole = np.load(PATH + 'resnet.npz')['arr_0']\n",
        "#     X = torch.Tensor(x_whole).reshape(x_whole.shape[0], -1)\n",
        "#     Y = torch.LongTensor(y_whole).reshape(y_whole.shape[0])\n",
        "#     meta = {'dim': X.shape[1], 'nClasses': 10}\n",
        "\n",
        "#     return X, Y, X.shape[0], meta['dim'], meta['nClasses']\n",
        "\n",
        "# X, Y, _, dim, C = get_imsat_stl10()\n",
        "\n",
        "# X = X.detach().cpu().numpy()\n",
        "# Y = Y.detach().cpu().numpy()\n",
        "# Y = Y.reshape(1,Y.shape[0])\n",
        "\n",
        "# print(dim)\n",
        "# print(C)\n",
        "# print(X.shape[0])\n",
        "\n",
        "# dsetname = \"stl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWDXVqpLLSHi"
      },
      "source": [
        "\n",
        "def conditional_entropy(soft):\n",
        "    loss = torch.sum(-soft*torch.log(soft + 1e-8)) / soft.shape[0]\n",
        "    return loss\n",
        "\n",
        "\n",
        "def entropy(soft):\n",
        "    avg_soft = torch.mean(soft, 0, True) \n",
        "    loss = -torch.sum(avg_soft * torch.log(avg_soft + 1e-8))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def kl(p, q):\n",
        "    loss = torch.sum(p * torch.log((p + 1e-8) / (q + 1e-8))) / p.shape[0]\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def _disable_tracking_bn_stats(model):\n",
        "\n",
        "    def switch_attr(m):\n",
        "        if hasattr(m, 'track_running_stats'):\n",
        "            m.track_running_stats ^= True\n",
        "            \n",
        "    model.apply(switch_attr)\n",
        "    yield\n",
        "    model.apply(switch_attr)\n",
        "\n",
        "\n",
        "def _l2_normalize(d):\n",
        "    d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
        "    d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-8\n",
        "    return d\n",
        "\n",
        "\n",
        "def return_vat_Loss(model, x, xi, eps):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    with _disable_tracking_bn_stats(model):\n",
        "        with torch.no_grad():\n",
        "            target = torch.softmax(model(x), 1) \n",
        "        \n",
        "        d = torch.randn(x.shape).to(dev)\n",
        "        d = _l2_normalize(d)\n",
        "        d.requires_grad_()\n",
        "        out_vadv = model(x + xi*d)\n",
        "        hat = torch.softmax(out_vadv, 1)\n",
        "        adv_distance = kl(target, hat)\n",
        "\n",
        "        adv_distance.backward()\n",
        "        \n",
        "        d = _l2_normalize(d.grad)\n",
        "        r_adv = eps * d\n",
        "        out_vadv = model(x + r_adv)\n",
        "        hat = torch.softmax(out_vadv, 1)\n",
        "        R_vat = kl(target, hat)\n",
        "\n",
        "    return R_vat\n",
        "\n",
        "\n",
        "\n",
        "def SiameseLoss(model, soft_out1, x_agm, t1, t2, r):\n",
        "\n",
        "    m = soft_out1.shape[0]\n",
        "    a = np.ones((m,m)) - np.eye(m)\n",
        "    a = csr_matrix(a)\n",
        "   \n",
        "    neg_idx_i, neg_idx_j = a.nonzero()\n",
        "    del a\n",
        "\n",
        "    l_neg = neg_idx_i.shape[0]\n",
        "    num_neg_pairs = int(l_neg*r)\n",
        "\n",
        "     \n",
        "    with _disable_tracking_bn_stats(model):\n",
        "        soft_out2 = torch.softmax(model(x_agm), 1)\n",
        "\n",
        "        s = np.random.choice(list(range(l_neg)), size=num_neg_pairs, replace=False)\n",
        "\n",
        "        neg_ip = soft_out1[neg_idx_i[s],:] * soft_out2[neg_idx_j[s],:]\n",
        "        neg_ip = torch.sum(neg_ip, 1)\n",
        "        neg_loss = torch.log(1 + t2*(1-neg_ip))\n",
        "        neg_loss = -torch.sum(neg_loss) / num_neg_pairs\n",
        "\n",
        "\n",
        "        pos_ip = soft_out1 * soft_out2\n",
        "        pos_ip = torch.sum(pos_ip, 1)\n",
        "        pos_loss = torch.log(1 + t1*(1-pos_ip))\n",
        "        pos_loss = torch.sum(pos_loss) / m\n",
        "\n",
        "\n",
        "\n",
        "    return pos_loss, neg_loss, soft_out2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ReturnACC(cluster, target_cluster, k):\n",
        "    \"\"\" Compute error between cluster and target cluster\n",
        "    :param cluster: proposed cluster\n",
        "    :param target_cluster: target cluster\n",
        "    k: number of classes\n",
        "    :return: error\n",
        "    \"\"\"\n",
        "    n = np.shape(target_cluster)[0]\n",
        "    M = np.zeros((k, k))\n",
        "    for i in range(k):\n",
        "        for j in range(k):\n",
        "            M[i][j] = np.sum(np.logical_and(cluster == i, target_cluster == j))\n",
        "    m = Munkres()\n",
        "    indexes = m.compute(-M)\n",
        "    corresp = []\n",
        "    for i in range(k):\n",
        "        corresp.append(indexes[i][1])\n",
        "    pred_corresp = [corresp[int(predicted)] for predicted in cluster]\n",
        "    acc = np.sum(pred_corresp == target_cluster) / float(len(target_cluster))\n",
        "    return acc \n",
        "\n",
        "\n",
        "\n",
        "def get_agm(x, idx_itr, knn_idx):\n",
        "    knn_idx_itr = knn_idx[idx_itr,:]\n",
        "    for i in range(knn_idx_itr.shape[0]):\n",
        "      v = np.random.permutation(knn_idx_itr[i,:])\n",
        "      knn_idx_itr[i,:] = v\n",
        "\n",
        "    v = np.random.choice(list(range(knn_idx_itr.shape[1])), size=1, replace=False)\n",
        "    idx0 = knn_idx_itr[:,v[0]]\n",
        "    x_agm0 = x[idx0,:]\n",
        "    \n",
        "    return x_agm0, idx0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K1MuvTn9eTQ"
      },
      "source": [
        "# ## Important two classes (annoy and nmslib) to conduct approximate knn\n",
        "# ## the below classes (NMSlibTransformer and AnnoyTransformer) are copied and pasted\n",
        "# ## from https://scikit-learn.org/stable/auto_examples/neighbors/approximate_nearest_neighbors.html\n",
        "\n",
        "# !pip install annoy nmslib\n",
        "\n",
        "# # Author: Tom Dupre la Tour\n",
        "# #\n",
        "# # License: BSD 3 clause\n",
        "# import time\n",
        "# import sys\n",
        "\n",
        "# try:\n",
        "#     import annoy\n",
        "# except ImportError:\n",
        "#     print(\"The package 'annoy' is required to run this example.\")\n",
        "#     sys.exit()\n",
        "\n",
        "# try:\n",
        "#     import nmslib\n",
        "# except ImportError:\n",
        "#     print(\"The package 'nmslib' is required to run this example.\")\n",
        "#     sys.exit()\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# from matplotlib.ticker import NullFormatter\n",
        "# from scipy.sparse import csr_matrix\n",
        "\n",
        "# from sklearn.base import BaseEstimator, TransformerMixin\n",
        "# from sklearn.neighbors import KNeighborsTransformer\n",
        "# from sklearn.utils._testing import assert_array_almost_equal\n",
        "# from sklearn.datasets import fetch_openml\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.manifold import TSNE\n",
        "# from sklearn.utils import shuffle\n",
        "\n",
        "# print(__doc__)\n",
        "\n",
        "\n",
        "# class NMSlibTransformer(TransformerMixin, BaseEstimator):\n",
        "#     \"\"\"Wrapper for using nmslib as sklearn's KNeighborsTransformer\"\"\"\n",
        "\n",
        "#     def __init__(self, n_neighbors=5, metric='euclidean', method='sw-graph',\n",
        "#                  n_jobs=1):\n",
        "#         self.n_neighbors = n_neighbors\n",
        "#         self.method = method\n",
        "#         self.metric = metric\n",
        "#         self.n_jobs = n_jobs\n",
        "\n",
        "#     def fit(self, X):\n",
        "#         self.n_samples_fit_ = X.shape[0]\n",
        "\n",
        "#         # see more metric in the manual\n",
        "#         # https://github.com/nmslib/nmslib/tree/master/manual\n",
        "#         space = {\n",
        "#             'sqeuclidean': 'l2',\n",
        "#             'euclidean': 'l2',\n",
        "#             'cosine': 'cosinesimil',\n",
        "#             'l1': 'l1',\n",
        "#             'l2': 'l2',\n",
        "#         }[self.metric]\n",
        "\n",
        "#         self.nmslib_ = nmslib.init(method=self.method, space=space)\n",
        "#         self.nmslib_.addDataPointBatch(X)\n",
        "#         self.nmslib_.createIndex()\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         n_samples_transform = X.shape[0]\n",
        "\n",
        "#         # For compatibility reasons, as each sample is considered as its own\n",
        "#         # neighbor, one extra neighbor will be computed.\n",
        "#         n_neighbors = self.n_neighbors + 1\n",
        "\n",
        "#         results = self.nmslib_.knnQueryBatch(X, k=n_neighbors,\n",
        "#                                              num_threads=self.n_jobs)\n",
        "#         indices, distances = zip(*results)\n",
        "#         indices, distances = np.vstack(indices), np.vstack(distances)\n",
        "\n",
        "#         if self.metric == 'sqeuclidean':\n",
        "#             distances **= 2\n",
        "\n",
        "#         indptr = np.arange(0, n_samples_transform * n_neighbors + 1,\n",
        "#                            n_neighbors)\n",
        "#         kneighbors_graph = csr_matrix((distances.ravel(), indices.ravel(),\n",
        "#                                        indptr), shape=(n_samples_transform,\n",
        "#                                                        self.n_samples_fit_))\n",
        "\n",
        "#         return kneighbors_graph\n",
        "\n",
        "\n",
        "# class AnnoyTransformer(TransformerMixin, BaseEstimator):\n",
        "#     \"\"\"Wrapper for using annoy.AnnoyIndex as sklearn's KNeighborsTransformer\"\"\"\n",
        "\n",
        "#     def __init__(self, n_neighbors=5, metric='euclidean', n_trees=10,\n",
        "#                  search_k=-1):\n",
        "#         self.n_neighbors = n_neighbors\n",
        "#         self.n_trees = n_trees\n",
        "#         self.search_k = search_k\n",
        "#         self.metric = metric\n",
        "\n",
        "#     def fit(self, X):\n",
        "#         self.n_samples_fit_ = X.shape[0]\n",
        "#         metric = self.metric if self.metric != 'sqeuclidean' else 'euclidean'\n",
        "#         self.annoy_ = annoy.AnnoyIndex(X.shape[1], metric=metric)\n",
        "#         for i, x in enumerate(X):\n",
        "#             self.annoy_.add_item(i, x.tolist())\n",
        "#         self.annoy_.build(self.n_trees)\n",
        "#         return self\n",
        "\n",
        "#     def transform(self, X):\n",
        "#         return self._transform(X)\n",
        "\n",
        "#     def fit_transform(self, X, y=None):\n",
        "#         return self.fit(X)._transform(X=None)\n",
        "\n",
        "#     def _transform(self, X):\n",
        "#         \"\"\"As `transform`, but handles X is None for faster `fit_transform`.\"\"\"\n",
        "\n",
        "#         n_samples_transform = self.n_samples_fit_ if X is None else X.shape[0]\n",
        "\n",
        "#         # For compatibility reasons, as each sample is considered as its own\n",
        "#         # neighbor, one extra neighbor will be computed.\n",
        "#         n_neighbors = self.n_neighbors + 1\n",
        "\n",
        "#         indices = np.empty((n_samples_transform, n_neighbors),\n",
        "#                            dtype=np.int)\n",
        "#         distances = np.empty((n_samples_transform, n_neighbors))\n",
        "\n",
        "#         if X is None:\n",
        "#             for i in range(self.annoy_.get_n_items()):\n",
        "#                 ind, dist = self.annoy_.get_nns_by_item(\n",
        "#                     i, n_neighbors, self.search_k, include_distances=True)\n",
        "\n",
        "#                 indices[i], distances[i] = ind, dist\n",
        "#         else:\n",
        "#             for i, x in enumerate(X):\n",
        "#                 indices[i], distances[i] = self.annoy_.get_nns_by_vector(\n",
        "#                     x.tolist(), n_neighbors, self.search_k,\n",
        "#                     include_distances=True)\n",
        "\n",
        "#         if self.metric == 'sqeuclidean':\n",
        "#             distances **= 2\n",
        "\n",
        "#         indptr = np.arange(0, n_samples_transform * n_neighbors + 1,\n",
        "#                            n_neighbors)\n",
        "#         kneighbors_graph = csr_matrix((distances.ravel(), indices.ravel(),\n",
        "#                                        indptr), shape=(n_samples_transform,\n",
        "#                                                        self.n_samples_fit_))\n",
        "\n",
        "#         return kneighbors_graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeTt8H309lNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef22b87-7397-49b9-ccfe-9fcf7dfc6142"
      },
      "source": [
        "import os\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "alpha = 0.25\n",
        "K = 200 # Number of neighbors\n",
        "\n",
        "if X.shape[0] > 5*10**5:\n",
        "  nms = NMSlibTransformer(n_neighbors = K) # Compute approximated knn graph\n",
        "  Knn_graph = nms.fit_transform(X)\n",
        "\n",
        "  # define adaptie radius for VAT\n",
        "  Knn_dist = Knn_graph.data.reshape(X.shape[0],K+1)\n",
        "  R = alpha*Knn_dist[:,K]\n",
        "  R = R.reshape(X.shape[0],1) \n",
        "\n",
        "  del Knn_graph, Knn_dist\n",
        "\n",
        "  end = time.time()\n",
        "  print(f\"{end-start} seconds by Approximated KNN.\")\n",
        "\n",
        "else:\n",
        "\n",
        "  knncachestr = \"%s-k%d.npy\" % (dsetname, K+1)\n",
        "  \n",
        "  if dsetname != \"unknown\" and os.path.exists(knncachestr):\n",
        "    \n",
        "    print(\"Loaded cached kNN from %s\" % knncachestr)\n",
        "    distances = np.load(knncachestr)\n",
        "    indices = np.load(\"i\" + knncachestr)\n",
        "  \n",
        "  else:\n",
        "\n",
        "    nbrs = NearestNeighbors(n_neighbors=K+1, algorithm='brute').fit(X)\n",
        "    distances, indices = nbrs.kneighbors(X)\n",
        "    np.save(knncachestr, distances)\n",
        "    np.save(\"i\" + knncachestr, indices)\n",
        "\n",
        "  K_vat = 10\n",
        "  R = alpha*distances[:,K_vat]\n",
        "  R = R.reshape(X.shape[0],1)\n",
        "\n",
        "\n",
        "  end = time.time()\n",
        "  print(f\"{end-start} seconds by Brute-Force KNN.\")\n",
        "\n",
        "\n",
        "\n",
        "R = torch.tensor(R.astype('f')).to(dev)\n",
        "\n",
        "\n",
        "X = torch.tensor(X.astype('f')).to(dev) # this unlabeled dataset (set of feature vectors) is input of IMSAT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded cached kNN from 20news-k201.npy\n",
            "0.016347408294677734 seconds by Brute-Force KNN.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM6hsDn18kgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e08adac-084d-4df6-aea4-b962e12df450"
      },
      "source": [
        "# define archtechture of MLP(Multi Layer Perceptron). \n",
        "# in this net, batch-normalization (bn) is used. \n",
        "# bn is very important to stabilize the training of net. \n",
        "#torch.manual_seed(0)\n",
        "class Net(nn.Module): \n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(dim, 1200)\n",
        "        self.bn1 = nn.BatchNorm1d(1200)\n",
        "\n",
        "        self.l2 = nn.Linear(1200,1200)\n",
        "        self.bn2 = nn.BatchNorm1d(1200)\n",
        "\n",
        "        self.l3 = nn.Linear(1200,C)\n",
        "        \n",
        "    def forward(self, x):\n",
        "       \n",
        "        x = F.relu(self.bn1(self.l1(x)))\n",
        "        x = F.relu(self.bn2(self.l2(x)))\n",
        "        x = self.l3(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "net = Net()\n",
        "\n",
        "# throw net object to gpu\n",
        "net = net.to(dev)\n",
        "\n",
        "\n",
        "################################################## Training of imsat ############################################\n",
        "\n",
        "# decide hyperparameter values for imsat training\n",
        "epochs = 50 # number of epochs\n",
        "\n",
        "xi = 10**(1)       # xi is used for making adversarial vector. if xi becomes smaller, \n",
        "                   # theoretically obtained r_vadv becomes more priecise\n",
        "\n",
        "\n",
        "mini_size = 250 # size of mini-batch training dataset\n",
        "\n",
        "m = X.shape[0]//mini_size # number of iteration at each epoch \n",
        "\n",
        "\n",
        "mu = 0.1\n",
        "gamma = 0.55\n",
        "eta1 = 5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## define optimizer for set of parameters in deep neural network\n",
        "## lr is the learning rate of parameter vector, betas are the lr of gradient and second moment of gradient\n",
        "optimizer = optim.Adam(net.parameters(), \n",
        "                        lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "\n",
        "\n",
        "## tau_pos = 2 or 5 is good for reuters, c=10,20 tau_pos =.75?\n",
        "tau_pos = 0.01\n",
        "tau_neg = tau_pos \n",
        "\n",
        "\n",
        "# given the number of negative, how much rate of them should be used for trainingm, rate=1\n",
        "# means 100% used\n",
        "rate = 1\n",
        "\n",
        "\n",
        "#itr_num = 0\n",
        "print(\"Start training of SIM_agm.\")\n",
        "for epoch in range(epochs):\n",
        "    print(\"At \", epoch, \"-th epoch, \")\n",
        "\n",
        "    # set empiricial loss to 0 in the beginning of epoch\n",
        "    empirical_loss = 0.0\n",
        "\n",
        "    idx_eph = np.random.permutation(X.shape[0])\n",
        "    \n",
        "    net.train()\n",
        "\n",
        "    for itr in range(m):\n",
        "      \n",
        "      ## chose a core idx of mini_batch\n",
        "      idx_itr = idx_eph[itr*mini_size:(itr+1)*mini_size]\n",
        "\n",
        "      # define components at each iteration\n",
        "      X_itr = X[idx_itr,:]\n",
        "\n",
        "\n",
        "      K0 = 5 #should be smaller than K=200, here we define T(x) for each x, K0 = 5, 10, 15, 20, 25, 50, 100, 150, 200(=K)\n",
        "      # X_agm1_itr, idx_agm1_itr = get_agm(X, idx_itr, indices[:,1:K0+1])\n",
        "      X_agm1_itr, idx_agm1_itr = get_agm(X, idx_itr, indices[:,int(2*K0/3):K0+1])\n",
        "      # X_agm1_itr, idx_agm1_itr = get_agm(X, idx_itr, indices[:,int(K0/2):K0+1])\n",
        "      \n",
        "\n",
        "\n",
        "      R_vat = return_vat_Loss(net, X_itr, xi, R[idx_itr,:])\n",
        "      R_vat1 = return_vat_Loss(net, X_agm1_itr, xi, R[idx_agm1_itr,:])\n",
        "\n",
        "\n",
        "\n",
        "      soft_out_itr = torch.softmax(net(X_itr) , 1)\n",
        "  \n",
        "\n",
        "      ## define positive and negative loss\n",
        "      l_p, l_n, soft_out_agm1_itr = SiameseLoss(net, soft_out_itr, X_agm1_itr, tau_pos, tau_neg, rate)\n",
        "\n",
        "      ## define entropy of y\n",
        "      ent_y = entropy(torch.cat((soft_out_itr, soft_out_agm1_itr),0))\n",
        "\n",
        "      ## define shannon conditional entropy loss H(p(y|x)) named by c_ent.\n",
        "      c_ent = conditional_entropy(torch.cat((soft_out_itr, soft_out_agm1_itr),0))\n",
        "      \n",
        "              \n",
        "      # objective of sim\n",
        "      eta2 = 1\n",
        "      objective = ((R_vat + R_vat1)/2) - mu*( (1-gamma)*(eta1*ent_y - c_ent) + gamma*eta2*(-l_p - l_n)  )\n",
        "\n",
        "\n",
        "\n",
        "      # update the set of parameters in deep neural network by minimizing loss\n",
        "      optimizer.zero_grad() \n",
        "      objective.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      empirical_loss = empirical_loss + objective.data\n",
        "\n",
        "\n",
        "\n",
        "    #empirical_loss = running_loss/m\n",
        "    empirical_loss = empirical_loss.cpu().numpy()\n",
        "    print(\"average empirical loss is\", empirical_loss/m, ',')\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    # at each epoch, prediction accuracy is displayed\n",
        "    with torch.no_grad():\n",
        "        out = net(X)\n",
        "        preds = torch.argmax(out, dim=1)\n",
        "        preds = preds.cpu().numpy()\n",
        "        preds = preds.reshape(1, preds.shape[0])\n",
        "        clustering_acc = ReturnACC(preds[0], Y[0], C)\n",
        "    print(\"and current clustering accuracy is\", clustering_acc )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training of SIM_agm.\n",
            "At  0 -th epoch, \n",
            "average empirical loss is -0.07775627772013347 ,\n",
            "and current clustering accuracy is 0.38315822986310094\n",
            "At  1 -th epoch, \n",
            "average empirical loss is -0.19015328725179037 ,\n",
            "and current clustering accuracy is 0.383741908097209\n",
            "At  2 -th epoch, \n",
            "average empirical loss is -0.37335840861002606 ,\n",
            "and current clustering accuracy is 0.3842725246736708\n",
            "At  3 -th epoch, \n",
            "average empirical loss is -0.49891138712565103 ,\n",
            "and current clustering accuracy is 0.38310516820545476\n",
            "At  4 -th epoch, \n",
            "average empirical loss is -0.548175557454427 ,\n",
            "and current clustering accuracy is 0.38299904489016234\n",
            "At  5 -th epoch, \n",
            "average empirical loss is -0.5786329142252604 ,\n",
            "and current clustering accuracy is 0.3821500583678234\n",
            "At  6 -th epoch, \n",
            "average empirical loss is -0.5992598470052083 ,\n",
            "and current clustering accuracy is 0.3820969967101772\n",
            "At  7 -th epoch, \n",
            "average empirical loss is -0.5982380167643229 ,\n",
            "and current clustering accuracy is 0.38931338215005834\n",
            "At  8 -th epoch, \n",
            "average empirical loss is -0.6138555908203125 ,\n",
            "and current clustering accuracy is 0.3962113976440624\n",
            "At  9 -th epoch, \n",
            "average empirical loss is -0.6143258158365885 ,\n",
            "and current clustering accuracy is 0.3965828292475857\n",
            "At  10 -th epoch, \n",
            "average empirical loss is -0.6175125122070313 ,\n",
            "and current clustering accuracy is 0.39605221267112384\n",
            "At  11 -th epoch, \n",
            "average empirical loss is -0.6239575703938802 ,\n",
            "and current clustering accuracy is 0.39610527432877\n",
            "At  12 -th epoch, \n",
            "average empirical loss is -0.6260809834798177 ,\n",
            "and current clustering accuracy is 0.3963175209593548\n",
            "At  13 -th epoch, \n",
            "average empirical loss is -0.6277757263183594 ,\n",
            "and current clustering accuracy is 0.39605221267112384\n",
            "At  14 -th epoch, \n",
            "average empirical loss is -0.6292104593912761 ,\n",
            "and current clustering accuracy is 0.3963175209593548\n",
            "At  15 -th epoch, \n",
            "average empirical loss is -0.631909434000651 ,\n",
            "and current clustering accuracy is 0.39557465775230816\n",
            "At  16 -th epoch, \n",
            "average empirical loss is -0.6322968546549479 ,\n",
            "and current clustering accuracy is 0.3959460893558315\n",
            "At  17 -th epoch, \n",
            "average empirical loss is -0.6355876668294271 ,\n",
            "and current clustering accuracy is 0.3956807810676006\n",
            "At  18 -th epoch, \n",
            "average empirical loss is -0.6352923583984375 ,\n",
            "and current clustering accuracy is 0.3959460893558315\n",
            "At  19 -th epoch, \n",
            "average empirical loss is -0.6367637634277343 ,\n",
            "and current clustering accuracy is 0.39599915101347766\n",
            "At  20 -th epoch, \n",
            "average empirical loss is -0.6395395914713542 ,\n",
            "and current clustering accuracy is 0.39599915101347766\n",
            "At  21 -th epoch, \n",
            "average empirical loss is -0.6404382832845052 ,\n",
            "and current clustering accuracy is 0.3958399660405391\n",
            "At  22 -th epoch, \n",
            "average empirical loss is -0.6376105244954428 ,\n",
            "and current clustering accuracy is 0.3954154727793696\n",
            "At  23 -th epoch, \n",
            "average empirical loss is -0.6361868286132812 ,\n",
            "and current clustering accuracy is 0.3954685344370158\n",
            "At  24 -th epoch, \n",
            "average empirical loss is -0.6432879638671875 ,\n",
            "and current clustering accuracy is 0.3954685344370158\n",
            "At  25 -th epoch, \n",
            "average empirical loss is -0.6382237752278646 ,\n",
            "and current clustering accuracy is 0.3954685344370158\n",
            "At  26 -th epoch, \n",
            "average empirical loss is -0.6375213114420573 ,\n",
            "and current clustering accuracy is 0.3954685344370158\n",
            "At  27 -th epoch, \n",
            "average empirical loss is -0.6396070353190104 ,\n",
            "and current clustering accuracy is 0.3954154727793696\n",
            "At  28 -th epoch, \n",
            "average empirical loss is -0.6426112365722656 ,\n",
            "and current clustering accuracy is 0.39536241112172343\n",
            "At  29 -th epoch, \n",
            "average empirical loss is -0.6420284016927084 ,\n",
            "and current clustering accuracy is 0.3954685344370158\n",
            "At  30 -th epoch, \n",
            "average empirical loss is -0.6413579305013021 ,\n",
            "and current clustering accuracy is 0.3951501644911387\n",
            "At  31 -th epoch, \n",
            "average empirical loss is -0.6430014546712239 ,\n",
            "and current clustering accuracy is 0.3947256712299692\n",
            "At  32 -th epoch, \n",
            "average empirical loss is -0.643302256266276 ,\n",
            "and current clustering accuracy is 0.39467260957232303\n",
            "At  33 -th epoch, \n",
            "average empirical loss is -0.6443028767903646 ,\n",
            "and current clustering accuracy is 0.39467260957232303\n",
            "At  34 -th epoch, \n",
            "average empirical loss is -0.644179433186849 ,\n",
            "and current clustering accuracy is 0.39461954791467685\n",
            "At  35 -th epoch, \n",
            "average empirical loss is -0.6435761006673177 ,\n",
            "and current clustering accuracy is 0.39467260957232303\n",
            "At  36 -th epoch, \n",
            "average empirical loss is -0.642814687093099 ,\n",
            "and current clustering accuracy is 0.3944603629417383\n",
            "At  37 -th epoch, \n",
            "average empirical loss is -0.6436159261067709 ,\n",
            "and current clustering accuracy is 0.3944603629417383\n",
            "At  38 -th epoch, \n",
            "average empirical loss is -0.6470771280924479 ,\n",
            "and current clustering accuracy is 0.3944603629417383\n",
            "At  39 -th epoch, \n",
            "average empirical loss is -0.6482575988769531 ,\n",
            "and current clustering accuracy is 0.39435423962644595\n",
            "At  40 -th epoch, \n",
            "average empirical loss is -0.6469612121582031 ,\n",
            "and current clustering accuracy is 0.3942481163111536\n",
            "At  41 -th epoch, \n",
            "average empirical loss is -0.6454434712727865 ,\n",
            "and current clustering accuracy is 0.39419505465350735\n",
            "At  42 -th epoch, \n",
            "average empirical loss is -0.6452231343587239 ,\n",
            "and current clustering accuracy is 0.3912766634829672\n",
            "At  43 -th epoch, \n",
            "average empirical loss is -0.6425731913248698 ,\n",
            "and current clustering accuracy is 0.39180728005942905\n",
            "At  44 -th epoch, \n",
            "average empirical loss is -0.6458881632486979 ,\n",
            "and current clustering accuracy is 0.39228483497824473\n",
            "At  45 -th epoch, \n",
            "average empirical loss is -0.649784444173177 ,\n",
            "and current clustering accuracy is 0.39228483497824473\n",
            "At  46 -th epoch, \n",
            "average empirical loss is -0.6490220133463541 ,\n",
            "and current clustering accuracy is 0.39223177332059855\n",
            "At  47 -th epoch, \n",
            "average empirical loss is -0.6472370910644532 ,\n",
            "and current clustering accuracy is 0.3919664650323676\n",
            "At  48 -th epoch, \n",
            "average empirical loss is -0.6463188171386719 ,\n",
            "and current clustering accuracy is 0.3923378966358909\n",
            "At  49 -th epoch, \n",
            "average empirical loss is -0.643184305826823 ,\n",
            "and current clustering accuracy is 0.39228483497824473\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}